{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "### Practical Session\n",
    "\n",
    "<br/> Prof. Dr. Georgios K. Ouzounis\n",
    "<br/> email: georgios.ouzounis@go.kauko.lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Data loading and pre-processing\n",
    "2. Building the RNN\n",
    "3. Train and deploy the RNN\n",
    "4. Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the past 10 years stocks of chosen company in my case it was AMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset description: the open high, low and close values of the AMD from 2009 to 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the the training dataset\n",
    "\n",
    "# load the file contents \n",
    "dataset_train = pd.read_csv('AMD_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/31/2009</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.65</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.66</td>\n",
       "      <td>14282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/3/2009</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.74</td>\n",
       "      <td>12875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/4/2009</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3.76</td>\n",
       "      <td>20095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/5/2009</td>\n",
       "      <td>3.77</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.75</td>\n",
       "      <td>12174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/6/2009</td>\n",
       "      <td>3.75</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.71</td>\n",
       "      <td>3.71</td>\n",
       "      <td>11660200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Open  High   Low  Close  Adj Close    Volume\n",
       "0  7/31/2009  3.66  3.73  3.65   3.66       3.66  14282400\n",
       "1   8/3/2009  3.75  3.78  3.70   3.74       3.74  12875200\n",
       "2   8/4/2009  3.75  3.85  3.68   3.76       3.76  20095900\n",
       "3   8/5/2009  3.77  3.79  3.70   3.75       3.75  12174500\n",
       "4   8/6/2009  3.75  3.79  3.66   3.71       3.71  11660200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if we got correct information from AMD_train.csv\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create a subtable of relevant entries (open values)\n",
    "# The .values makes this vector a numpy array\n",
    "training_set = dataset_train.iloc[:, 1:2].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.66    ],\n",
       "       [ 3.75    ],\n",
       "       [ 3.75    ],\n",
       "       ...,\n",
       "       [34.23    ],\n",
       "       [34.139999],\n",
       "       [33.209999]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays do not support the view() and head() methods. [More on accessing the numpy data](https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Next we need to rescale our data to the range from 0 to 1. \n",
    "\n",
    "Feature scaling is essential as discussed if the Features lecture and needs to be applied to both the training and test sets.\n",
    "\n",
    "It is computed using the ScikitLearn library [MinMaxScaler()](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler) which transforms the selected feature by scaling it to a given range. If more than one, this estimator scales and translates each feature individually such that it is in the given range on the training set, i.e. between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# import the MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaler instance to rescale all data to the range of 0.0 to 1.0 \n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual training set of scaled values\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0624235 ],\n",
       "       [0.06517748],\n",
       "       [0.06517748],\n",
       "       ...,\n",
       "       [0.99785805],\n",
       "       [0.99510404],\n",
       "       [0.96664627]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training set to dependent and independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1bckuLGZCeLUzNA-xJCGOODzC-4n2U-If\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure with 90 timesteps and 1 output\n",
    "\n",
    "# the 90 stock prices in the last 3 months before today\n",
    "X_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2516, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I think that we check for maximum range for \n",
    "training_set_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stock price today\n",
    "y_train = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start from day 90 because that is the first instance allowing us to \n",
    "# go back 90 days\n",
    "for i in range(90, 2516): \n",
    "    # 0 is the column ID, the only column in this case.\n",
    "    # put the last 90 days values in one row of X_train\n",
    "    X_train.append(training_set_scaled[i-90:i, 0]) \n",
    "    y_train.append(training_set_scaled[i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0624235 , 0.06517748, 0.06517748, ..., 0.1759486 , 0.19798042,\n",
       "        0.20287638],\n",
       "       [0.06517748, 0.06517748, 0.06578948, ..., 0.19798042, 0.20287638,\n",
       "        0.20838434],\n",
       "       [0.06517748, 0.06578948, 0.06517748, ..., 0.20287638, 0.20838434,\n",
       "        0.22001225],\n",
       "       ...,\n",
       "       [0.67258264, 0.76101594, 0.73929014, ..., 0.96542224, 0.97980416,\n",
       "        0.99235006],\n",
       "       [0.76101594, 0.73929014, 0.79314571, ..., 0.97980416, 0.99235006,\n",
       "        0.99785805],\n",
       "       [0.73929014, 0.79314571, 0.75489601, ..., 0.99235006, 0.99785805,\n",
       "        0.99510404]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to add the stock value of somebody else together with the the past 60 days of Google, we need to change the length of the 3 dimension to  2.  RNN training tables are 3D!!! Read: [Reshaping NumPy Array | Numpy Array Reshape Examples](https://backtobazics.com/python/python-reshaping-numpy-array-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the data matrix, we retain the 2 original dimensions and add a third of depth=1\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN initialization\n",
    "\n",
    "- Import the sequential model from the Keras API;\n",
    "- Import the Dense layer template from the Keras API;\n",
    "- Import the LSTM model from the Keras API\n",
    "- Create an instance of the sequential model called regressor because we want to predict a continuous value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN as a sequence of layers\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add First Layer\n",
    "\n",
    "We first add an object of the LSTM class! \n",
    "\n",
    "- The first argument is the number of units or LSTM memory cells. Include many neurons to address the high dimensionality of the problem; say 50 neurons! \n",
    "- Second arg: return sequences = true; stacked LSTM !\n",
    "- Third arg: input 3D shape: observations vs time steps vs number of indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape =  (X_train.shape[1], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the argument is the dropout rate to ignore in the layers (20%), \n",
    "# i.e. 50 units * 20% = 10 units will be dropped each time\n",
    "regressor.add(Dropout(0.2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add More Layers\n",
    "\n",
    "We can add more LSTM layers but along with Dropout regularization to make sure we avoid overfitting! \n",
    "\n",
    "We don’t need to add the shape of the layer again because it is recognized automatically from the number of input units.\n",
    "\n",
    "The last layer does not return a sequence but connected directly to a fully connected output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "# we removed the return_sequences because we no longer return a \n",
    "# sequence but a value instead\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Output Layer & Compile\n",
    "\n",
    "The output has 1 dimension , i.e. one value to be predicted thus or output fully connected layer has dimensionality = 1.\n",
    "\n",
    "- **Optimizer**: rmsprop is recommended in the Keras documentation. The Adam optimizer is also a powerful choice.\n",
    "- **Loss function**: regression problems take the mean square error as most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and deploy the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the Training set\n",
    "\n",
    "We now want to train our RNN using the data in our **Training Set X** and **predictors in y** (ground truth in this case). Parameters that can be specified are the:\n",
    "\n",
    "- **Batch size**:  update the cell weights not on every stock price on every batch_size values; \n",
    "- **Number of epochs**: how many iterations to be used, i.e. number of forward and backward propagations for the update of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "76/76 [==============================] - 17s 227ms/step - loss: 0.0070\n",
      "Epoch 2/100\n",
      "76/76 [==============================] - 16s 212ms/step - loss: 0.0030\n",
      "Epoch 3/100\n",
      "76/76 [==============================] - 12s 152ms/step - loss: 0.0024\n",
      "Epoch 4/100\n",
      "76/76 [==============================] - 10s 133ms/step - loss: 0.0025\n",
      "Epoch 5/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 0.0022\n",
      "Epoch 6/100\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.0017\n",
      "Epoch 7/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 0.0017\n",
      "Epoch 8/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 0.0015\n",
      "Epoch 9/100\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.0015\n",
      "Epoch 10/100\n",
      "76/76 [==============================] - 10s 133ms/step - loss: 0.0014\n",
      "Epoch 11/100\n",
      "76/76 [==============================] - 10s 133ms/step - loss: 0.0016\n",
      "Epoch 12/100\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.0020\n",
      "Epoch 13/100\n",
      "76/76 [==============================] - 10s 136ms/step - loss: 0.0014\n",
      "Epoch 14/100\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.0015\n",
      "Epoch 15/100\n",
      "76/76 [==============================] - 10s 133ms/step - loss: 0.0013\n",
      "Epoch 16/100\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.0014\n",
      "Epoch 17/100\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.0014\n",
      "Epoch 18/100\n",
      "76/76 [==============================] - 10s 134ms/step - loss: 0.0012\n",
      "Epoch 19/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 0.0012\n",
      "Epoch 20/100\n",
      "76/76 [==============================] - 11s 144ms/step - loss: 0.0014\n",
      "Epoch 21/100\n",
      "76/76 [==============================] - 21s 271ms/step - loss: 0.0013\n",
      "Epoch 22/100\n",
      "76/76 [==============================] - 19s 256ms/step - loss: 0.0014\n",
      "Epoch 23/100\n",
      "76/76 [==============================] - 16s 215ms/step - loss: 0.0012\n",
      "Epoch 24/100\n",
      "76/76 [==============================] - 12s 152ms/step - loss: 0.0013\n",
      "Epoch 25/100\n",
      "76/76 [==============================] - 12s 155ms/step - loss: 0.0010\n",
      "Epoch 26/100\n",
      "76/76 [==============================] - 12s 157ms/step - loss: 0.0012\n",
      "Epoch 27/100\n",
      "76/76 [==============================] - 11s 150ms/step - loss: 0.0011\n",
      "Epoch 28/100\n",
      "76/76 [==============================] - 11s 151ms/step - loss: 9.7661e-04\n",
      "Epoch 29/100\n",
      "76/76 [==============================] - 11s 150ms/step - loss: 0.0010\n",
      "Epoch 30/100\n",
      "76/76 [==============================] - 12s 152ms/step - loss: 0.0011\n",
      "Epoch 31/100\n",
      "76/76 [==============================] - 11s 148ms/step - loss: 0.0011\n",
      "Epoch 32/100\n",
      "76/76 [==============================] - 11s 143ms/step - loss: 8.9983e-04\n",
      "Epoch 33/100\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 0.0011\n",
      "Epoch 34/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 0.0011\n",
      "Epoch 35/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 9.4587e-04\n",
      "Epoch 36/100\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 9.3614e-04\n",
      "Epoch 37/100\n",
      "76/76 [==============================] - 10s 136ms/step - loss: 9.8188e-04\n",
      "Epoch 38/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 8.6803e-04\n",
      "Epoch 39/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 0.0010\n",
      "Epoch 40/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 8.5507e-04\n",
      "Epoch 41/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 8.5729e-04\n",
      "Epoch 42/100\n",
      "76/76 [==============================] - 11s 143ms/step - loss: 8.7299e-04\n",
      "Epoch 43/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 8.7101e-04\n",
      "Epoch 44/100\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 8.7558e-04\n",
      "Epoch 45/100\n",
      "76/76 [==============================] - 12s 152ms/step - loss: 8.6681e-04\n",
      "Epoch 46/100\n",
      "76/76 [==============================] - 11s 148ms/step - loss: 8.9477e-04\n",
      "Epoch 47/100\n",
      "76/76 [==============================] - 12s 156ms/step - loss: 8.1464e-04\n",
      "Epoch 48/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 8.3522e-04\n",
      "Epoch 49/100\n",
      "76/76 [==============================] - 11s 147ms/step - loss: 8.6961e-04\n",
      "Epoch 50/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 8.1190e-04\n",
      "Epoch 51/100\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 8.4393e-04\n",
      "Epoch 52/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 7.7627e-04\n",
      "Epoch 53/100\n",
      "76/76 [==============================] - 11s 142ms/step - loss: 8.4140e-04\n",
      "Epoch 54/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 8.3101e-04\n",
      "Epoch 55/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 8.3870e-04\n",
      "Epoch 56/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 8.0099e-04\n",
      "Epoch 57/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 7.1121e-04\n",
      "Epoch 58/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 8.6223e-04\n",
      "Epoch 59/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 8.6829e-04\n",
      "Epoch 60/100\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 9.3407e-04\n",
      "Epoch 61/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 7.5627e-04\n",
      "Epoch 62/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 9.1359e-04\n",
      "Epoch 63/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 7.3466e-04\n",
      "Epoch 64/100\n",
      "76/76 [==============================] - 12s 154ms/step - loss: 8.0730e-04\n",
      "Epoch 65/100\n",
      "76/76 [==============================] - 11s 145ms/step - loss: 7.2327e-04\n",
      "Epoch 66/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 7.8466e-04\n",
      "Epoch 67/100\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 7.6038e-04\n",
      "Epoch 68/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 7.9628e-04\n",
      "Epoch 69/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 9.4415e-04\n",
      "Epoch 70/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 7.2597e-04\n",
      "Epoch 71/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 8.2367e-04\n",
      "Epoch 72/100\n",
      "76/76 [==============================] - 12s 154ms/step - loss: 7.7903e-04\n",
      "Epoch 73/100\n",
      "76/76 [==============================] - 12s 159ms/step - loss: 8.3061e-04\n",
      "Epoch 74/100\n",
      "76/76 [==============================] - 12s 154ms/step - loss: 8.0680e-04\n",
      "Epoch 75/100\n",
      "76/76 [==============================] - 13s 177ms/step - loss: 7.9538e-04\n",
      "Epoch 76/100\n",
      "76/76 [==============================] - 12s 153ms/step - loss: 6.7699e-04\n",
      "Epoch 77/100\n",
      "76/76 [==============================] - 12s 153ms/step - loss: 6.8970e-04\n",
      "Epoch 78/100\n",
      "76/76 [==============================] - 12s 152ms/step - loss: 6.3667e-04\n",
      "Epoch 79/100\n",
      "76/76 [==============================] - 11s 149ms/step - loss: 6.7126e-04\n",
      "Epoch 80/100\n",
      "76/76 [==============================] - 11s 140ms/step - loss: 7.0387e-04\n",
      "Epoch 81/100\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 7.5144e-04\n",
      "Epoch 82/100\n",
      "76/76 [==============================] - 11s 143ms/step - loss: 7.0135e-04\n",
      "Epoch 83/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 6.1011e-04\n",
      "Epoch 84/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 6.4453e-04\n",
      "Epoch 85/100\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 7.1567e-04\n",
      "Epoch 86/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 6.6681e-04\n",
      "Epoch 87/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 7.2748e-04\n",
      "Epoch 88/100\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 6.6113e-04\n",
      "Epoch 89/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 6.7301e-04\n",
      "Epoch 90/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 6.7702e-04\n",
      "Epoch 91/100\n",
      "76/76 [==============================] - 11s 139ms/step - loss: 6.6321e-04\n",
      "Epoch 92/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 8.2927e-04\n",
      "Epoch 93/100\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 6.6387e-04\n",
      "Epoch 94/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 7.4763e-04\n",
      "Epoch 95/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 6.8843e-04\n",
      "Epoch 96/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 6.2406e-04\n",
      "Epoch 97/100\n",
      "76/76 [==============================] - 10s 136ms/step - loss: 6.5915e-04\n",
      "Epoch 98/100\n",
      "76/76 [==============================] - 10s 137ms/step - loss: 6.7632e-04\n",
      "Epoch 99/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 6.7484e-04\n",
      "Epoch 100/100\n",
      "76/76 [==============================] - 10s 138ms/step - loss: 5.3849e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdf1c668460>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Letting the program about sets of training and predictions\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Predictions\n",
    "\n",
    "Create a data-frame by importing the Google Stock Price Test set for January 2017 using pandas and make it a numpy array.\n",
    "\n",
    "There are 20 financial days in one month, weekends are excluded!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/1/2019</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>31.480000</td>\n",
       "      <td>29.100000</td>\n",
       "      <td>29.860001</td>\n",
       "      <td>29.860001</td>\n",
       "      <td>80878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8/2/2019</td>\n",
       "      <td>29.480000</td>\n",
       "      <td>29.730000</td>\n",
       "      <td>28.940001</td>\n",
       "      <td>29.440001</td>\n",
       "      <td>29.440001</td>\n",
       "      <td>60410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/5/2019</td>\n",
       "      <td>28.260000</td>\n",
       "      <td>28.490000</td>\n",
       "      <td>27.650000</td>\n",
       "      <td>27.990000</td>\n",
       "      <td>27.990000</td>\n",
       "      <td>74333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/6/2019</td>\n",
       "      <td>28.860001</td>\n",
       "      <td>29.049999</td>\n",
       "      <td>28.200001</td>\n",
       "      <td>28.860001</td>\n",
       "      <td>28.860001</td>\n",
       "      <td>60578400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/7/2019</td>\n",
       "      <td>28.469999</td>\n",
       "      <td>29.280001</td>\n",
       "      <td>28.370001</td>\n",
       "      <td>29.190001</td>\n",
       "      <td>29.190001</td>\n",
       "      <td>58577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8/8/2019</td>\n",
       "      <td>31.530001</td>\n",
       "      <td>34.270000</td>\n",
       "      <td>31.480000</td>\n",
       "      <td>33.919998</td>\n",
       "      <td>33.919998</td>\n",
       "      <td>167278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8/9/2019</td>\n",
       "      <td>33.450001</td>\n",
       "      <td>35.549999</td>\n",
       "      <td>33.080002</td>\n",
       "      <td>34.189999</td>\n",
       "      <td>34.189999</td>\n",
       "      <td>132483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8/12/2019</td>\n",
       "      <td>34.160000</td>\n",
       "      <td>34.650002</td>\n",
       "      <td>32.080002</td>\n",
       "      <td>32.430000</td>\n",
       "      <td>32.430000</td>\n",
       "      <td>106737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8/13/2019</td>\n",
       "      <td>32.360001</td>\n",
       "      <td>33.139999</td>\n",
       "      <td>31.719999</td>\n",
       "      <td>32.110001</td>\n",
       "      <td>32.110001</td>\n",
       "      <td>102009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8/14/2019</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.049999</td>\n",
       "      <td>29.510000</td>\n",
       "      <td>30.240000</td>\n",
       "      <td>30.240000</td>\n",
       "      <td>127521500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8/15/2019</td>\n",
       "      <td>30.629999</td>\n",
       "      <td>30.730000</td>\n",
       "      <td>29.209999</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>71674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8/16/2019</td>\n",
       "      <td>30.309999</td>\n",
       "      <td>31.480000</td>\n",
       "      <td>30.209999</td>\n",
       "      <td>31.180000</td>\n",
       "      <td>31.180000</td>\n",
       "      <td>70469800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8/19/2019</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.189999</td>\n",
       "      <td>31.420000</td>\n",
       "      <td>31.480000</td>\n",
       "      <td>31.480000</td>\n",
       "      <td>67596900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8/20/2019</td>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.309999</td>\n",
       "      <td>30.450001</td>\n",
       "      <td>30.719999</td>\n",
       "      <td>30.719999</td>\n",
       "      <td>47924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8/21/2019</td>\n",
       "      <td>31.049999</td>\n",
       "      <td>31.740000</td>\n",
       "      <td>30.840000</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>41441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8/22/2019</td>\n",
       "      <td>31.760000</td>\n",
       "      <td>31.920000</td>\n",
       "      <td>30.980000</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>47667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8/23/2019</td>\n",
       "      <td>31.299999</td>\n",
       "      <td>31.830000</td>\n",
       "      <td>29.400000</td>\n",
       "      <td>29.540001</td>\n",
       "      <td>29.540001</td>\n",
       "      <td>83681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8/26/2019</td>\n",
       "      <td>30.340000</td>\n",
       "      <td>30.719999</td>\n",
       "      <td>29.940001</td>\n",
       "      <td>30.280001</td>\n",
       "      <td>30.280001</td>\n",
       "      <td>50612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8/27/2019</td>\n",
       "      <td>30.600000</td>\n",
       "      <td>30.889999</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>30.200001</td>\n",
       "      <td>30.200001</td>\n",
       "      <td>53229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8/28/2019</td>\n",
       "      <td>29.920000</td>\n",
       "      <td>31.180000</td>\n",
       "      <td>29.700001</td>\n",
       "      <td>30.780001</td>\n",
       "      <td>30.780001</td>\n",
       "      <td>55835900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8/29/2019</td>\n",
       "      <td>31.469999</td>\n",
       "      <td>31.830000</td>\n",
       "      <td>31.330000</td>\n",
       "      <td>31.450001</td>\n",
       "      <td>31.450001</td>\n",
       "      <td>44422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8/30/2019</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>31.870001</td>\n",
       "      <td>31.129999</td>\n",
       "      <td>31.450001</td>\n",
       "      <td>31.450001</td>\n",
       "      <td>40307000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close  \\\n",
       "0    8/1/2019  30.500000  31.480000  29.100000  29.860001  29.860001   \n",
       "1    8/2/2019  29.480000  29.730000  28.940001  29.440001  29.440001   \n",
       "2    8/5/2019  28.260000  28.490000  27.650000  27.990000  27.990000   \n",
       "3    8/6/2019  28.860001  29.049999  28.200001  28.860001  28.860001   \n",
       "4    8/7/2019  28.469999  29.280001  28.370001  29.190001  29.190001   \n",
       "5    8/8/2019  31.530001  34.270000  31.480000  33.919998  33.919998   \n",
       "6    8/9/2019  33.450001  35.549999  33.080002  34.189999  34.189999   \n",
       "7   8/12/2019  34.160000  34.650002  32.080002  32.430000  32.430000   \n",
       "8   8/13/2019  32.360001  33.139999  31.719999  32.110001  32.110001   \n",
       "9   8/14/2019  31.000000  31.049999  29.510000  30.240000  30.240000   \n",
       "10  8/15/2019  30.629999  30.730000  29.209999  29.670000  29.670000   \n",
       "11  8/16/2019  30.309999  31.480000  30.209999  31.180000  31.180000   \n",
       "12  8/19/2019  32.000000  32.189999  31.420000  31.480000  31.480000   \n",
       "13  8/20/2019  30.940001  31.309999  30.450001  30.719999  30.719999   \n",
       "14  8/21/2019  31.049999  31.740000  30.840000  31.700001  31.700001   \n",
       "15  8/22/2019  31.760000  31.920000  30.980000  31.900000  31.900000   \n",
       "16  8/23/2019  31.299999  31.830000  29.400000  29.540001  29.540001   \n",
       "17  8/26/2019  30.340000  30.719999  29.940001  30.280001  30.280001   \n",
       "18  8/27/2019  30.600000  30.889999  29.600000  30.200001  30.200001   \n",
       "19  8/28/2019  29.920000  31.180000  29.700001  30.780001  30.780001   \n",
       "20  8/29/2019  31.469999  31.830000  31.330000  31.450001  31.450001   \n",
       "21  8/30/2019  31.600000  31.870001  31.129999  31.450001  31.450001   \n",
       "\n",
       "       Volume  \n",
       "0    80878900  \n",
       "1    60410900  \n",
       "2    74333200  \n",
       "3    60578400  \n",
       "4    58577500  \n",
       "5   167278800  \n",
       "6   132483900  \n",
       "7   106737000  \n",
       "8   102009700  \n",
       "9   127521500  \n",
       "10   71674400  \n",
       "11   70469800  \n",
       "12   67596900  \n",
       "13   47924000  \n",
       "14   41441500  \n",
       "15   47667900  \n",
       "16   83681100  \n",
       "17   50612500  \n",
       "18   53229200  \n",
       "19   55835900  \n",
       "20   44422800  \n",
       "21   40307000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the real stock price for August1st 20019 - \n",
    "# August 31st 2019\n",
    "\n",
    "dataset_test = pd.read_csv('AMD_Test.csv')\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for how many real stocks there are\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "real_stock_price.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.5     ],\n",
       "       [29.48    ],\n",
       "       [28.26    ],\n",
       "       [28.860001],\n",
       "       [28.469999],\n",
       "       [31.530001],\n",
       "       [33.450001],\n",
       "       [34.16    ],\n",
       "       [32.360001],\n",
       "       [31.      ],\n",
       "       [30.629999],\n",
       "       [30.309999],\n",
       "       [32.      ],\n",
       "       [30.940001],\n",
       "       [31.049999],\n",
       "       [31.76    ],\n",
       "       [31.299999],\n",
       "       [30.34    ],\n",
       "       [30.6     ],\n",
       "       [29.92    ],\n",
       "       [31.469999],\n",
       "       [31.6     ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for opening numbers\n",
    "real_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the stock price value for each day in January 2017, we need the values in the last 60 days.\n",
    "\n",
    "To obtain this **history** we need to combine both the training and test sets in one.\n",
    "\n",
    "If we were to use the training_set and test_set we would need to use the scaler  but that would change the actual test values.  Thus concatenate the original data frames!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2019\n",
    "\n",
    "# axis = 0 means concatenate the lines (i.e. vertical axis)\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for how many entries there are.\n",
    "dataset_total.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the difference in the length of the first two gives us \n",
    "# the first day in 2019, and we need to go back 90 days to get the necessary range\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 90:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we did not use iloc from panda so lets reshape the numpy array for \n",
    "# compatibility: i.e. all the values from input lines to be stacked in one \n",
    "# column. The -1 means that the numpy has no knowledge of how the \n",
    "# values were stored in lines. The 1 means we want to them in one \n",
    "# column.\n",
    "\n",
    "inputs = inputs.reshape(-1,1) \n",
    "\n",
    "# apply the feature scaler\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted stock price of 2019\n",
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 90 from inputs are from training set; start \n",
    "# from 90 and get the extra 22, i.e. up to 112\n",
    "for i in range(90, 112): \n",
    "    X_test.append(inputs[i-90:i, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3D structure\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to inverse the scaling to get meaningful predicted stock price # outputs\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price) \n",
    "predicted_stock_price.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real AMD Stock Price')\n",
    "\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted AMD Stock Price')\n",
    "\n",
    "plt.title('AMD Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('AMD Stock Price')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3998140191908426"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what is the difference(the perfect outcome is getting 0)\n",
    "#\n",
    "rmse = math.sqrt( mean_squared_error( real_stock_price[0:22,:], predicted_stock_price))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data need to be placed in the same order/format  as in the case of the training/test sets.\n",
    "\n",
    "1. Getting more training data: we trained our model on the past 5 years of the  Google Stock Price but it would be even better to train it on the past 10 years.\n",
    "\n",
    "2. Increasing the number of time steps: the model remembered the stock price from the 60 previous financial days to predict the stock price of the next day. That’s because we chose a number of 60 time steps (3 months). You could try to increase the number of time steps, by choosing for example 120 time steps (6 months).\n",
    "\n",
    "3. Adding some other indicators: if you have the financial instinct that the stock price of some other companies might be correlated to the one of Google, you could add this other stock price as a new indicator in the training data.\n",
    "\n",
    "4. Adding more LSTM layers: we built a RNN with four LSTM layers but you could try with even more.\n",
    "\n",
    "5. Adding more neurons in the LSTM layers: we highlighted the fact that we needed a high number of neurons in the LSTM layers to respond better to the complexity of the problem and we chose to include 50 neurons in each of our 4 LSTM layers. You could try an architecture with even more neurons in each of the 4 (or more) LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the RNN\n",
    "\n",
    "Parameter Tuning on the RNN model: we are dealing with a Regression problem because we predict a continuous outcome.\n",
    "\n",
    "**Tip**: replace: scoring = 'accuracy' by scoring = 'neg_mean_squared_error' in the GridSearchCV class parameters as we did in the ANN case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
